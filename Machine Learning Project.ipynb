{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce15aa6",
   "metadata": {},
   "source": [
    "<center><h1>Machine Learning Project with kNN, DT, LR and RF Classifiers</h1>\n",
    "<h2>Itgel Ganbold</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d5c8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T14:25:47.788349Z",
     "start_time": "2023-10-04T14:25:47.784106Z"
    }
   },
   "source": [
    "## Task 1: \n",
    "- Load the Dry_Bean_dataset.csv and test the performance of the following models:\n",
    "    - a. k-Nearest Neighbour\n",
    "    - b. Decision Tree\n",
    "    - c. Logistic Regression\n",
    "    - d. Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f8595",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:19:39.562019Z",
     "start_time": "2023-10-25T21:19:38.957138Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a93134",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:19:40.598288Z",
     "start_time": "2023-10-25T21:19:40.029433Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "beans_df = pd.read_csv('Dry_Bean_Dataset.csv')\n",
    "beans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ecc502",
   "metadata": {},
   "source": [
    "First, let's check if any data cleaning is necessary. To do this, I will check if there are any missing values for any of the features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f9eca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:19:42.089877Z",
     "start_time": "2023-10-25T21:19:42.076345Z"
    }
   },
   "outputs": [],
   "source": [
    "beans_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a3ead",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:19:42.953087Z",
     "start_time": "2023-10-25T21:19:42.829062Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "beans_df['Class'].value_counts().plot(kind='barh', color='green')\n",
    "plt.title('Frequency Bar Chart for \"Class\" Feature')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='x')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0ca01",
   "metadata": {},
   "source": [
    "We see that the different classes are not equally represented. This could lead to issues where the models become biased towards the majority class. For this reason, we may be cognizant of certain performance metrics which better handle class-imbalance, such as precision and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064911c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:19:44.130598Z",
     "start_time": "2023-10-25T21:19:44.120185Z"
    }
   },
   "outputs": [],
   "source": [
    "y = beans_df.pop('Class')\n",
    "X = beans_df.values\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e889b73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:19:44.748475Z",
     "start_time": "2023-10-25T21:19:44.722298Z"
    }
   },
   "outputs": [],
   "source": [
    "beans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f3432e",
   "metadata": {},
   "source": [
    "### a.        k-nearest neightbour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6199f86",
   "metadata": {},
   "source": [
    "#### Normalize the data as we have mixed scales. Some features have no units.\n",
    "\n",
    " To normalize, we have two options, $N(0,1)$ and `MinMax` scaling. To know which one to choose, we can check for outliers and if there are significant outliers, we should not use `MinMax` as outliers will skew our normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07beb9ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:19:46.402509Z",
     "start_time": "2023-10-25T21:19:46.359744Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = beans_df.columns\n",
    "\n",
    "for column in columns:\n",
    "    # Calculate the IQR (Interquartile Range) for Column1\n",
    "    Q1 = beans_df[column].quantile(0.25)\n",
    "    Q3 = beans_df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define lower and upper bounds to identify outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Check for outliers\n",
    "    outliers = beans_df[( beans_df[column] < lower_bound) | ( beans_df[column] > upper_bound)]\n",
    "\n",
    "    # Print outliers\n",
    "    print(column, ' has ' , round(100*outliers.shape[0]/beans_df.shape[0], 2), '% outliers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131afb90",
   "metadata": {},
   "source": [
    "We can visually confirm this by plotting the frequency plot for each feature. We can see the outliers, distribution variance, the shape and the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592d5bec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:19:49.979148Z",
     "start_time": "2023-10-25T21:19:47.362089Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=6, ncols=3, figsize=(16, 22))\n",
    "for i, column in enumerate(beans_df.columns):  \n",
    "    sns.histplot(beans_df[column], ax=axes[i//3, i%3], bins=100, kde=True)\n",
    "    axes[i//3, i%3].set_title(column, fontsize=16)\n",
    "    axes[i//3, i%3].grid(True)\n",
    "    axes[i//3, i%3].set_axisbelow(True)\n",
    "\n",
    "for j in range(i+1, 6*3):\n",
    "    fig.delaxes(axes.flatten()[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca05ad0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T14:52:32.794894Z",
     "start_time": "2023-10-04T14:52:32.788674Z"
    }
   },
   "source": [
    "Using 1.5 as a multiplier for the IQR we see that many of the features exhibit some outliers, and thus $N(0,1)$ maybe a more suitable normalization approach, even if some of the features don't quite look like Gaussian. We can use other ways to measure outliers, such as figuring out the maximum $\\sigma$, i.e. $z$-score for each features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ddbf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:19:51.526160Z",
     "start_time": "2023-10-25T21:19:51.516605Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9623096",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:19:52.201646Z",
     "start_time": "2023-10-25T21:19:52.196622Z"
    }
   },
   "outputs": [],
   "source": [
    "X_scaled_df = pd.DataFrame(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee603dcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:19:55.612931Z",
     "start_time": "2023-10-25T21:19:52.926623Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(\"### Feature frequency plot after $N(0,1)$ Gaussian normalization.\"))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=6, ncols=3, figsize=(16, 22))\n",
    "for i, column in enumerate(X_scaled_df):  \n",
    "    sns.histplot(X_scaled_df[column], ax=axes[i//3, i%3], bins=100, kde=True)\n",
    "    axes[i//3, i%3].set_title(beans_df.columns[column], fontsize=16)\n",
    "    axes[i//3, i%3].grid(True)\n",
    "    axes[i//3, i%3].set_axisbelow(True)\n",
    "\n",
    "for j in range(i+1, 6*3):\n",
    "    fig.delaxes(axes.flatten()[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf275b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:19:58.791463Z",
     "start_time": "2023-10-25T21:19:58.778276Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f7337f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:20:01.638636Z",
     "start_time": "2023-10-25T21:19:59.662077Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = []\n",
    "acc_train = []\n",
    "\n",
    "pres = []\n",
    "pres_train = []\n",
    "\n",
    "for i in range(10, 91, 10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=(i/100), random_state=100)\n",
    "    kNN = KNeighborsClassifier()\n",
    "    kNN = kNN.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = kNN.predict(X_test)\n",
    "    y_pred_train = kNN.predict(X_train)\n",
    "\n",
    "    acc.append(accuracy_score(y_test, y_pred))\n",
    "    acc_train.append(accuracy_score(y_train, y_pred_train))\n",
    "    \n",
    "    pres.append(precision_score(y_test, y_pred, average='weighted', zero_division=1))\n",
    "    pres_train.append(precision_score(y_train, y_pred_train, average='weighted', zero_division=1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1c499",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:20:10.280145Z",
     "start_time": "2023-10-25T21:20:10.154151Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = range(90,9,-10)\n",
    "plt.plot(x, acc, label = 'Accuracy on test data')\n",
    "plt.plot(x, acc_train, label = 'Accuracy on training data')\n",
    "plt.title(\"Model Accuracy vs Training Set %\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Training set %')\n",
    "plt.ylabel('Model accuracy rate')\n",
    "plt.ylim(0.9, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a0fdf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:20:11.409330Z",
     "start_time": "2023-10-25T21:20:11.298361Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, pres, label = 'Precision on test data')\n",
    "plt.plot(x, pres_train, label = 'Precision on training data')\n",
    "plt.title(\"Model Precision vs Training Set %\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Training set %')\n",
    "plt.ylabel('Model accuracy rate')\n",
    "plt.ylim(0.9, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e6c38",
   "metadata": {},
   "source": [
    "We see that changing the test-train split make very little difference to the `kNN` model accuracy and precision. This is quite unusual as even only using 10% of the dataset for training is informative enough to give rather high accuracy and precision scores. Next, lets look at how the different number of nearest neighbor selection affects the overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c45188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:20:22.543318Z",
     "start_time": "2023-10-25T21:20:16.402182Z"
    }
   },
   "outputs": [],
   "source": [
    "acc_difference = []\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=(0.2), random_state=100)\n",
    "\n",
    "for i in range(1, 30):\n",
    "    kNN = KNeighborsClassifier(n_neighbors=i)\n",
    "\n",
    "    kNN = kNN.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = kNN.predict(X_test)\n",
    "    y_pred_train = kNN.predict(X_train)\n",
    "    \n",
    "    acc_difference.append(accuracy_score(y_train, y_pred_train) - accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af5385d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:20:23.534231Z",
     "start_time": "2023-10-25T21:20:23.416207Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = range(1,30)\n",
    "plt.plot(x, acc_difference)\n",
    "plt.title(\"Test set and train set accuracy difference\")\n",
    "plt.scatter(x, acc_difference, zorder = 5)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel('Number of nearest neighbours')\n",
    "plt.ylabel('Evaluation metric rate')\n",
    "# plt.ylim(0.9, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e489aea0",
   "metadata": {},
   "source": [
    "We can see that `n_neighbors` of 17 gives a good result. This tracks well with the rule of thumb that the number of neighbors roughly equaling the number of features in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd1e8bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:20:26.439037Z",
     "start_time": "2023-10-25T21:20:26.419280Z"
    }
   },
   "outputs": [],
   "source": [
    "kNN = KNeighborsClassifier(n_neighbors=17)\n",
    "kNN = kNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7b43f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:20:28.663491Z",
     "start_time": "2023-10-25T21:20:28.402326Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = kNN.predict(X_test)\n",
    "\n",
    "model_metrics = classification_report(y_test, y_pred, target_names=kNN.classes_)\n",
    "print(model_metrics)\n",
    "\n",
    "\n",
    "#Calculate the confusion matrix\n",
    "confus_matrix_values = confusion_matrix(y_test, y_pred)\n",
    "confus_matrix = confusion_matrix(y_test, y_pred, normalize=\"pred\")\n",
    "print(\"\\nConfusion Matrix:\\n\", '-'*30,'\\n')\n",
    "display(Markdown(\"### Confusion Matrix: \"))\n",
    "print(confus_matrix_values)\n",
    "display(Markdown(\"### Confusion Matrix Precision: \"))\n",
    "disp = ConfusionMatrixDisplay(confus_matrix, display_labels=kNN.classes_)\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc2746",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note: The Confusion matrices shown in markdown throughout this notebook may differ slightly upon rerunning of the relevant cells in the notebook.</b>\n",
    "</div>\n",
    "\n",
    "#### The confusion matrix is as follows: \n",
    "\n",
    "\n",
    "|          | BARBUNYA | BOMBAY | CALI | DERMASON | HOROZ | SEKER | SIRA | Total (Actual) |\n",
    "|----------|----------|--------|------|----------|-------|-------|------|-------|\n",
    "| BARBUNYA | 242      | 0      | 19   | 0        | 0     | 1     | 8    | **270**   |\n",
    "| BOMBAY   | 0        | 103    | 0    | 0        | 0     | 0     | 0    | **103**   |\n",
    "| CALI     | 8        | 0      | 316  | 0        | 7     | 0     | 2    | **333**   |\n",
    "| DERMASON | 0       | 0      | 0    | 660      | 0     | 5     | 40   | **705**   |\n",
    "| HOROZ    | 0       | 0      | 7    | 1        | 371   | 0     | 7    | **386**   |\n",
    "| SEKER    | 2       | 0      | 0    | 11       | 0     | 376   | 16   | **405**   |\n",
    "| SIRA     | 1       | 0      | 1    | 38       | 11    | 6     | 464  | **521**   |\n",
    "| **Total (Predicted)**| **253** | **103**| **343**| **710**| **389**| **388**| **537**| **2723**|\n",
    "\n",
    "The diagonal values are the correct classifications, i.e. True Positives. We also note that Bombay had perfect classification as all 103 (this value will differ between reruns of the nb) predicted Bombay beans were indeed actual Bombay beans.\n",
    "\n",
    "Note that for the holdout testing, I split the dataset using 80:20 split. This seem to result in good weighted averaged evaluation metrics. This is what I will be looking at more in this notebook, instead of considering the metrics for individual classes as shown previously.\n",
    "\n",
    "|**Metric**|**Value**|\n",
    "|----------|---------|\n",
    "|Accuracy  |0.927 |\n",
    "|Precision |0.928|\n",
    "|Recall    |0.927|\n",
    "|F1-score  |0.927|\n",
    "\n",
    "\n",
    "The formula for these metrics: \n",
    "\n",
    "1. Accuracy: The ratio of correctly predicted instances to the total instances. \n",
    "\n",
    "<center> $\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$ </center>\n",
    "\n",
    "2. Precision: The ratio of correctly predicted positive observations to the total predicted positives for each class.\n",
    "\n",
    "<center> $\\text{Overall Precision} = \\frac{\\sum \\text{(True Positives (class) } \\times \\text{Total (Actual class))}}{\\sum \\text{Total (Predicted class)}}$ </center>\n",
    "\n",
    "3. Recall: The ratio of correctly predicted positive observations to all the actual positives for each class.\n",
    "\n",
    "<center> $\\text{Overall Recall} = \\frac{\\sum \\text{(True Positives (class) } \\times \\text{Total (Actual class))}}{\\sum \\text{Total (Actual class)}}$</center>\n",
    "\n",
    "4. F1 Score: The harmonic mean of precision and recall for that class.\n",
    "\n",
    "<center> $\\text{Overall F1-Score} = 2 \\times \\frac{\\text{Overall Precision } \\times \\text{Overall Recall}}{\\text{Overall Precision } + \\text{Overall Recall}}$ </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1899aba9",
   "metadata": {},
   "source": [
    "#### Check for overfitting:\n",
    "\n",
    "I will check again how well the model generalizes the dataset by comparing the accuracy of the prediction result of the training set with the test set. If the results differ significantly, it could indicate overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1073d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:20:43.979018Z",
     "start_time": "2023-10-25T21:20:43.785433Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_train = kNN.predict(X_train)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Accuracy when using the training set:\", round(accuracy_train, 4))\n",
    "print(\"Accuracy when using the test set: \", round(accuracy, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af3561",
   "metadata": {},
   "source": [
    "We can see that the model is able to generalize well as the prediction results for the training set is not significantly more than the result we got using the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8c69d",
   "metadata": {},
   "source": [
    "### b.     Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84f4f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:20:49.047100Z",
     "start_time": "2023-10-25T21:20:49.006482Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3788f757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:20:50.120552Z",
     "start_time": "2023-10-25T21:20:49.768241Z"
    }
   },
   "outputs": [],
   "source": [
    "#For the decision tree criterion, we can either use gini or entropy. \n",
    "#I will compare the performance of each criterion to select the better option\n",
    "dTree_entropy = DecisionTreeClassifier(criterion='entropy')\n",
    "dTree_entropy = dTree_entropy.fit(X_train, y_train)\n",
    "\n",
    "dTree_gini = DecisionTreeClassifier(criterion='gini')\n",
    "dTree_gini = dTree_gini.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4029c79b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:20:50.775921Z",
     "start_time": "2023-10-25T21:20:50.578885Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_dtree_entropy = dTree_entropy.predict(X_test)\n",
    "\n",
    "display(Markdown(\"#### Weighted model performance metrics:\"))\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy_dtree_entropy = accuracy_score(y_test, y_pred_dtree_entropy)\n",
    "print(\"Accuracy:\", round(accuracy_dtree_entropy, 3))\n",
    "\n",
    "# Calculate and print precision\n",
    "precision_dtree_entropy = precision_score(y_test, y_pred_dtree_entropy, average='weighted', zero_division=1)\n",
    "print(\"Precision:\", round(precision_dtree_entropy,3))\n",
    "\n",
    "# Calculate and print recall\n",
    "recall_dtree_entropy = recall_score(y_test, y_pred_dtree_entropy, average='weighted')\n",
    "print(\"Recall:\", round(recall_dtree_entropy,3))\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1_dtree_entropy = f1_score(y_test, y_pred_dtree_entropy, average='weighted')\n",
    "print(\"F1 Score:\", round(f1_dtree_entropy,3), '\\n')\n",
    "\n",
    "#Calculate the confusion matrix\n",
    "confus_matrix_dtree_entropy = confusion_matrix(y_test, y_pred_dtree_entropy, normalize=\"pred\")\n",
    "display(Markdown(\"#### Desision Tree Precision Confusion Matrix (entropy):\"))\n",
    "disp = ConfusionMatrixDisplay(confus_matrix_dtree_entropy, display_labels=kNN.classes_)\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168dee2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:20:59.731717Z",
     "start_time": "2023-10-25T21:20:59.546654Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_dtree_gini = dTree_gini.predict(X_test)\n",
    "\n",
    "display(Markdown(\"#### Weighted model performance metrics:\"))\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy_dtree_gini = accuracy_score(y_test, y_pred_dtree_gini)\n",
    "print(\"Accuracy:\", round(accuracy_dtree_gini, 3))\n",
    "\n",
    "# Calculate and print precision\n",
    "precision_dtree_gini = precision_score(y_test, y_pred_dtree_gini, average='weighted', zero_division=1)\n",
    "print(\"Precision:\", round(precision_dtree_gini,3))\n",
    "\n",
    "# Calculate and print recall\n",
    "recall_dtree_gini = recall_score(y_test, y_pred_dtree_gini, average='weighted')\n",
    "print(\"Recall:\", round(recall_dtree_gini,3))\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1_dtree_gini = f1_score(y_test, y_pred_dtree_gini, average='weighted')\n",
    "print(\"F1 Score:\", round(f1_dtree_gini,3), '\\n')\n",
    "\n",
    "#Calculate the confusion matrix\n",
    "confus_matrix_dtree_gini = confusion_matrix(y_test, y_pred_dtree_gini, normalize=\"pred\")\n",
    "display(Markdown(\"#### Desision Tree Precision Confusion Matrix (gini):\"))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confus_matrix_dtree_gini, display_labels=kNN.classes_)\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2413c8c",
   "metadata": {},
   "source": [
    "#### Confusion Martrix for Decision Tree with 'entropy' criterion:\n",
    "\n",
    "|            | BARBUNYA | BOMBAY | CALI | DERMASON | HOROZ | SEKER | SIRA | **Total (Actual)**   |\n",
    "|------------|----------|--------|------|----------|-------|-------|------|-------------|\n",
    "| BARBUNYA   | 242      | 0      | 20   | 1        | 1     | 2     | 4    | **270**     |\n",
    "| BOMBAY     | 0        | 103    | 0    | 0        | 0     | 0     | 0    | **103**     |\n",
    "| CALI       | 26       | 0      | 301  | 0        | 5     | 1     | 0    | **333**     |\n",
    "| DERMASON   | 0        | 0      | 0    | 633      | 6     | 7     | 59   | **705**     |\n",
    "| HOROZ      | 4        | 0      | 6    | 1        | 362   | 0     | 13   | **386**     |\n",
    "| SEKER      | 4        | 0      | 0    | 14       | 0     | 372   | 15   | **405**     |\n",
    "| SIRA       | 2        | 0      | 4    | 54       | 13    | 15    | 433  | **521**     |\n",
    "| **Total (Predicted)**  | **278**  | **103**|**331**|**703**  |**387**|**397**|**524**|**2723**    |   \n",
    "\n",
    "Evaluation metric: \n",
    "| Metric    | Value |\n",
    "|-----------|-------|\n",
    "| Accuracy  | 0.898 |\n",
    "| Precision | 0.899 |\n",
    "| Recall    | 0.898 |\n",
    "| F1 Score  | 0.898 |\n",
    "\n",
    "\n",
    "\n",
    "#### Confusion Matrix for Decision Tree with 'gini' criterion:\n",
    "\n",
    "|            | BARBUNYA | BOMBAY | CALI | DERMASON | HOROZ | SEKER | SIRA | **Total (Actual)**   |\n",
    "|------------|----------|--------|------|----------|-------|-------|------|-------------|\n",
    "| BARBUNYA   | 238      | 0      | 20   | 1        | 2     | 2     | 7    | **270**     |\n",
    "| BOMBAY     | 0        | 103    | 0    | 0        | 0     | 0     | 0    | **103**     |\n",
    "| CALI       | 25       | 0      | 300  | 0        | 3     | 1     | 4    | **333**     |\n",
    "| DERMASON   | 0        | 0      | 0    | 639      | 3     | 10    | 53   | **705**     |\n",
    "| HOROZ      | 3        | 0      | 4    | 1        | 363   | 0     | 15   | **386**     |\n",
    "| SEKER      | 3        | 0      | 1    | 10       | 0     | 377   | 14   | **405**     |\n",
    "| SIRA       | 4        | 0      | 2    | 50       | 17    | 13    | 435  | **521**     |\n",
    "| **Total (Predicted)**  | **273**  | **103**|**327**|**701**  |**388**|**403**|**528**|**2723**    |\n",
    "\n",
    "Evaluation metrics: \n",
    "| Metric    | Value |\n",
    "|-----------|-------|\n",
    "| Accuracy  | 0.902 |\n",
    "| Precision | 0.902 |\n",
    "| Recall    | 0.902 |\n",
    "| F1 Score  | 0.902 |\n",
    "\n",
    "\n",
    "We can clearly see that using the 'gini' criterion is ever-so-slightly better than the 'entropy' criterion. \n",
    "Next, let's check how the model responds to various test-train splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fba1234",
   "metadata": {},
   "source": [
    "#### Check for model overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca604d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:21:06.595682Z",
     "start_time": "2023-10-25T21:21:06.571233Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_train_dt = dTree_gini.predict(X_train)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train_dt)\n",
    "print(\"Accuracy when using the training set:\", round(accuracy_train, 4))\n",
    "print(\"Accuracy when using the test set: \", round(accuracy_dtree_gini, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb24c13",
   "metadata": {},
   "source": [
    "We see that the model has learned the training set completely, as we are getting 100% accuracy. This means I need to tune some of the parameters so that the model is able to generalize the problem better. I will adjust the `max_depth` parameter to reduce the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e001848b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:21:07.969096Z",
     "start_time": "2023-10-25T21:21:07.963501Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Tree depth until leaf nodes: \", dTree_gini.tree_.max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d741c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:21:12.278468Z",
     "start_time": "2023-10-25T21:21:09.093298Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_dt_depth = []\n",
    "accuracy_dt_depth_test = []\n",
    "depth = []\n",
    "for i in range(1,26):\n",
    "    depth.append(i)\n",
    "    dTree_gini_temp = DecisionTreeClassifier(criterion='gini', max_depth=i)\n",
    "    dTree_gini_temp = dTree_gini_temp.fit(X_train, y_train)\n",
    "    y_pred_train_temp = dTree_gini_temp.predict(X_train)\n",
    "    \n",
    "    y_pred_test_temp = dTree_gini_temp.predict(X_test)\n",
    "    \n",
    "    accuracy_dt_depth_val = accuracy_score(y_train, y_pred_train_temp)\n",
    "    accuracy_dt_depth_test_val = accuracy_score(y_test, y_pred_test_temp)\n",
    "    accuracy_dt_depth.append(accuracy_dt_depth_val)\n",
    "    accuracy_dt_depth_test.append(accuracy_dt_depth_test_val)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f5b88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:21:12.412049Z",
     "start_time": "2023-10-25T21:21:12.281267Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(depth, accuracy_dt_depth, label = 'Training set', zorder = 5)\n",
    "plt.plot(depth, accuracy_dt_depth_test, label = 'Test set')\n",
    "plt.scatter(depth, accuracy_dt_depth, zorder = 5)\n",
    "plt.scatter(depth, accuracy_dt_depth_test)\n",
    "plt.title(\"Train and Test set Accuracy vs Model depth\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('Model accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6c80ef",
   "metadata": {},
   "source": [
    "We see from the above plot that `max_depth` of 5 gives maximum accuracy where the test set and training set results are nearly identical. After this, the two diverge, meaning, the model has started to overfit significantly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387726c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:21:18.522499Z",
     "start_time": "2023-10-25T21:21:18.268147Z"
    }
   },
   "outputs": [],
   "source": [
    "dTree_gini = DecisionTreeClassifier(criterion='gini', max_depth=5)\n",
    "dTree_gini = dTree_gini.fit(X_train, y_train)\n",
    "\n",
    "display(Markdown(\"#### Weighted performance metrics:\"))\n",
    "# Make predictions on the test set\n",
    "y_pred_dtree_gini = dTree_gini.predict(X_test)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy_dtree_gini = accuracy_score(y_test, y_pred_dtree_gini)\n",
    "print(\"Accuracy:\", round(accuracy_dtree_gini, 3))\n",
    "\n",
    "# Calculate and print precision\n",
    "precision_dtree_gini = precision_score(y_test, y_pred_dtree_gini, average='weighted', zero_division=1)\n",
    "print(\"Precision:\", round(precision_dtree_gini,3))\n",
    "\n",
    "# Calculate and print recall\n",
    "recall_dtree_gini = recall_score(y_test, y_pred_dtree_gini, average='weighted')\n",
    "print(\"Recall:\", round(recall_dtree_gini,3))\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1_dtree_gini = f1_score(y_test, y_pred_dtree_gini, average='weighted')\n",
    "print(\"F1 Score:\", round(f1_dtree_gini,3), '\\n')\n",
    "\n",
    "#Calculate the confusion matrix\n",
    "confus_matrix_dtree_gini = confusion_matrix(y_test, y_pred_dtree_gini, normalize=\"pred\")\n",
    "display(Markdown(\"#### Desision Tree Precision Confusion Matrix (gini):\"))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confus_matrix_dtree_gini, display_labels=kNN.classes_)\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39180d5",
   "metadata": {},
   "source": [
    "#### Confusion Matrix for Decision Tree model with `max_depth` of 5: \n",
    "|            | BARBUNYA | BOMBAY | CALI | DERMASON | HOROZ | SEKER | SIRA | **Total (Actual)**   |\n",
    "|------------|----------|--------|------|----------|-------|-------|------|-------------|\n",
    "| BARBUNYA   | 177      | 0      | 81   | 0        | 0     | 1     | 11   | **270**     |\n",
    "| BOMBAY     | 0        | 103    | 0    | 0        | 0     | 0     | 0    | **103**     |\n",
    "| CALI       | 29       | 0      | 303  | 0        | 1     | 0     | 0    | **333**     |\n",
    "| DERMASON   | 0        | 0      | 0    | 656      | 0     | 4     | 45   | **705**     |\n",
    "| HOROZ      | 0        | 0      | 17   | 2        | 348   | 0     | 19   | **386**     |\n",
    "| SEKER      | 3        | 0      | 0    | 12       | 0     | 372   | 18   | **405**     |\n",
    "| SIRA       | 0        | 0      | 5    | 47       | 2     | 6     | 461  | **521**     |\n",
    "| **Total (Predicted)**  | **209**  | **103**|**406**|**717**  |**351**|**383**|**554**|**2723**    |\n",
    "\n",
    "Performance metrics on test set: \n",
    "\n",
    "| Metric    | Value |\n",
    "|-----------|-------|\n",
    "| Accuracy  | 0.889 |\n",
    "| Precision | 0.894 |\n",
    "| Recall    | 0.889 |\n",
    "| F1 Score  | 0.889 |\n",
    "\n",
    "Next, lets consider the various test-train splits, to see if there is an optimal value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78e2e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:21:22.762245Z",
     "start_time": "2023-10-25T21:21:21.942768Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = []\n",
    "acc_train = []\n",
    "\n",
    "pres = []\n",
    "pres_train = []\n",
    "\n",
    "x = []\n",
    "\n",
    "for i in range(10, 91, 10):\n",
    "    x.append(100 - i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=(i/100), random_state=100)\n",
    "    dTree_gini = DecisionTreeClassifier(criterion='gini', max_depth=5)\n",
    "    dTree_gini = dTree_gini.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = dTree_gini.predict(X_test)\n",
    "    y_pred_train = dTree_gini.predict(X_train)\n",
    "\n",
    "    acc.append(accuracy_score(y_test, y_pred))\n",
    "    acc_train.append(accuracy_score(y_train, y_pred_train))\n",
    "    \n",
    "    pres.append(precision_score(y_test, y_pred, average='weighted', zero_division=1))\n",
    "    pres_train.append(precision_score(y_train, y_pred_train, average='weighted', zero_division=1))\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841fd8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:21:27.413930Z",
     "start_time": "2023-10-25T21:21:27.286426Z"
    }
   },
   "outputs": [],
   "source": [
    "# x = range(90, 9, -10)\n",
    "\n",
    "plt.plot(x, acc)\n",
    "plt.plot(x, acc_train)\n",
    "plt.scatter(x, acc, label = 'Accuracy on test data')\n",
    "plt.scatter(x, acc_train, label = 'Accuracy on training data')\n",
    "\n",
    "\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Training set %')\n",
    "plt.ylabel('Accuracy rate')\n",
    "plt.ylim(0.8, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14df31",
   "metadata": {},
   "source": [
    "The most optimal test-train split appears to be 40:60, our split of 20:80 is a bit less. However, if we compare the differences, the accuracy differs by about 0.008, or 0.8%. As this value is incredibly low, I will keep the split of 80:20 for the training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4498f66",
   "metadata": {},
   "source": [
    "### c.   Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83032bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:21:36.793439Z",
     "start_time": "2023-10-25T21:21:36.788457Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37ca35d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:21:37.991808Z",
     "start_time": "2023-10-25T21:21:37.264864Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=100)\n",
    "logis_reg = LogisticRegression(max_iter=10000, random_state=1)\n",
    "logis_reg = logis_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced200fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:21:38.266409Z",
     "start_time": "2023-10-25T21:21:38.014019Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_logistic = logis_reg.predict(X_test)\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Weighted model performance metrics:\"))\n",
    "# Calculate and print accuracy\n",
    "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)\n",
    "print(\"Accuracy:\", round(accuracy_logistic, 3))\n",
    "\n",
    "# Calculate and print precision\n",
    "precision_logistic = precision_score(y_test, y_pred_logistic, average='weighted', zero_division=1)\n",
    "print(\"Precision:\", round(precision_logistic,3))\n",
    "\n",
    "# Calculate and print recall\n",
    "recall_logistic = recall_score(y_test, y_pred_logistic, average='weighted')\n",
    "print(\"Recall:\", round(recall_logistic,3))\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1_logistic = f1_score(y_test, y_pred_logistic, average='weighted')\n",
    "print(\"F1 Score:\", round(f1_logistic,3), '\\n')\n",
    "\n",
    "#Calculate the confusion matrix\n",
    "confus_matrix_logistic = confusion_matrix(y_test, y_pred_logistic, normalize=\"pred\")\n",
    "display(Markdown(\"#### Precision Confusion Matrix for Logistic Regression: \"))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confus_matrix_logistic, display_labels=kNN.classes_)\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48fdc03",
   "metadata": {},
   "source": [
    "#### Confusion Matrix for LogisticRegression:\n",
    "\n",
    "|            | BARBUNYA | BOMBAY | CALI | DERMASON | HOROZ | SEKER | SIRA | **Total (Actual)**   |\n",
    "|------------|----------|--------|------|----------|-------|-------|------|-------------|\n",
    "| BARBUNYA   | 245      | 0      | 14   | 0        | 0     | 1     | 10   | **270**     |\n",
    "| BOMBAY     | 0        | 103    | 0    | 0        | 0     | 0     | 0    | **103**     |\n",
    "| CALI       | 12       | 0      | 315  | 0        | 5     | 0     | 1    | **333**     |\n",
    "| DERMASON   | 1        | 0      | 0    | 648      | 2     | 6     | 48   | **705**     |\n",
    "| HOROZ      | 0        | 0      | 6    | 2        | 372   | 0     | 6    | **386**     |\n",
    "| SEKER      | 4        | 0      | 0    | 8        | 0     | 378   | 15   | **405**     |\n",
    "| SIRA       | 1        | 0      | 0    | 38       | 11    | 8     | 463  | **521**     |\n",
    "| **Total (Predicted)**  | **263**  | **103**|**335**|**696**  |**390**|**393**|**543**|**2723**    |\n",
    "\n",
    "with performance metrics:\n",
    "| Metric    | Value |\n",
    "|-----------|-------|\n",
    "| Accuracy  | 0.927 |\n",
    "| Precision | 0.928 |\n",
    "| Recall    | 0.927 |\n",
    "| F1 Score  | 0.927 |\n",
    "\n",
    "#### Check for model overfitting:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d464c87e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:21:43.930174Z",
     "start_time": "2023-10-25T21:21:43.557532Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_logistic_train = logis_reg.predict(X_train)\n",
    "display(Markdown(\"#### Model performance metrics on training set: \"))\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy_logistic_train = accuracy_score(y_train, y_pred_logistic_train)\n",
    "print(\"Accuracy:\", round(accuracy_logistic_train, 3))\n",
    "\n",
    "# Calculate and print precision\n",
    "precision_logistic_train = precision_score(y_train, y_pred_logistic_train, average='weighted', zero_division=1)\n",
    "print(\"Precision:\", round(precision_logistic_train,3))\n",
    "\n",
    "# Calculate and print recall\n",
    "recall_logistic_train = recall_score(y_train, y_pred_logistic_train, average='weighted')\n",
    "print(\"Recall:\", round(recall_logistic_train,3))\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1_logistic_train = f1_score(y_train, y_pred_logistic_train, average='weighted')\n",
    "print(\"F1 Score:\", round(f1_logistic_train,3), '\\n')\n",
    "\n",
    "#Calculate the confusion matrix\n",
    "confus_matrix_logistic_train = confusion_matrix(y_train, y_pred_logistic_train, normalize=\"pred\")\n",
    "display(Markdown(\"#### Precision Confusion Matrix for Training Set: \"))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confus_matrix_logistic_train, display_labels=kNN.classes_)\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd0b9f",
   "metadata": {},
   "source": [
    "We can see from the performance metric that the model is able to generalize well as the results on the training set is consistent with the result we got using the test set. Next, I will check for any optimal values we could use for the test-train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1db048f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:21:51.527211Z",
     "start_time": "2023-10-25T21:21:47.203365Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = []\n",
    "acc_train = []\n",
    "x = []\n",
    "\n",
    "for i in range(10, 91, 10):\n",
    "    x.append(100 - i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=(i/100), random_state=100)\n",
    "    \n",
    "    log_reg = LogisticRegression(max_iter=10000, random_state=100)\n",
    "    log_reg = log_reg.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = log_reg.predict(X_test)\n",
    "    y_pred_train = log_reg.predict(X_train)\n",
    "\n",
    "    acc.append(accuracy_score(y_test, y_pred))\n",
    "    acc_train.append(accuracy_score(y_train, y_pred_train))\n",
    "    \n",
    "    pres.append(precision_score(y_test, y_pred, average='weighted', zero_division=1))\n",
    "    pres_train.append(precision_score(y_train, y_pred_train, average='weighted', zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b0727",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:21:56.165868Z",
     "start_time": "2023-10-25T21:21:56.044786Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, acc, label = 'Accuracy on test data')\n",
    "plt.plot(x, acc_train, label = 'Accuracy on training data')\n",
    "plt.scatter(x, acc)\n",
    "plt.scatter(x, acc_train)\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Training set %')\n",
    "plt.ylabel('Accuracy rate')\n",
    "plt.ylim(0.91, 0.94);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a73d15e",
   "metadata": {},
   "source": [
    "As before, the optimal train % is 40%, however, the differences between these splits are extremely small. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db0d5c",
   "metadata": {},
   "source": [
    "### d. RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26da217",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:21:58.765676Z",
     "start_time": "2023-10-25T21:21:58.704945Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c49bc25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:22:01.744438Z",
     "start_time": "2023-10-25T21:21:59.291193Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=100)\n",
    "rand_for = RandomForestClassifier(criterion = 'gini')\n",
    "rand_for = rand_for.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34bb61e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:22:14.353968Z",
     "start_time": "2023-10-25T21:22:14.147705Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_rf = rand_for.predict(X_test)\n",
    "\n",
    "display(Markdown(\"#### Model weighted performance metrics: \"))\n",
    "# Calculate and print accuracy\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Accuracy:\", round(accuracy_rf, 3))\n",
    "\n",
    "# Calculate and print precision\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average='weighted', zero_division=1)\n",
    "print(\"Precision:\", round(precision_rf,3))\n",
    "\n",
    "# Calculate and print recall\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "print(\"Recall:\", round(recall_rf,3))\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "print(\"F1 Score:\", round(f1_rf,3), '\\n')\n",
    "\n",
    "#Calculate the confusion matrix\n",
    "confus_rf = confusion_matrix(y_test, y_pred_rf, normalize=\"pred\")\n",
    "display(Markdown(\"#### Random Forest Precision Confusion Matrix: \"))\n",
    "disp = ConfusionMatrixDisplay(confus_rf, display_labels=kNN.classes_)\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b5aa43",
   "metadata": {},
   "source": [
    "RandomForest has 3 types of criterions: 'gini', 'entropy' and 'log_loss'\n",
    "I will do a quick test to see which gives better performance. The default is 'gini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5193ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:22:29.284068Z",
     "start_time": "2023-10-25T21:22:21.263817Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = ['gini', 'log_loss', 'entropy']\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "\n",
    "\n",
    "for critery in criterion:\n",
    "    rand_for = RandomForestClassifier(criterion=critery)\n",
    "    rand_for = rand_for.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_rf = rand_for.predict(X_test)\n",
    "    \n",
    "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "    acc.append(accuracy_rf)\n",
    "\n",
    "    precision_rf = precision_score(y_test, y_pred_rf, average='weighted', zero_division=1)\n",
    "    prec.append(precision_rf)\n",
    "\n",
    "    recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "    rec.append(recall_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6f212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:22:30.298364Z",
     "start_time": "2023-10-25T21:22:30.179756Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(criterion, acc, label = 'Accuracy')\n",
    "plt.scatter(criterion, prec, label = 'Precision')\n",
    "plt.scatter(criterion, rec, label = 'Recall')\n",
    "plt.legend()\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba1ad1c",
   "metadata": {},
   "source": [
    "We see from the above graph that the 'entropy' criterion is the best, although not by much compared to the others. \n",
    "\n",
    "Different runs of my notebook results in different outcomes. Given that these results are so close to each other, choosing the right criterion seems not that important.\n",
    "\n",
    "Another metric that play an import role in Decision tree models is the number of estimators `n_estimators`. The default value is 100 as of version 0.22. So, I will also find the optimal value for `n`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9da3c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:23:01.382162Z",
     "start_time": "2023-10-25T21:22:34.196198Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = []\n",
    "prec = []\n",
    "rec = []    \n",
    "\n",
    "\n",
    "for n in range(1,101,5):\n",
    "    rand_for = RandomForestClassifier(n_estimators=n, criterion='entropy')\n",
    "    rand_for = rand_for.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_rf = rand_for.predict(X_test)\n",
    "    \n",
    "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "    acc.append(accuracy_rf)\n",
    "\n",
    "    precision_rf = precision_score(y_test, y_pred_rf, average='weighted', zero_division=1)\n",
    "    prec.append(precision_rf)\n",
    "\n",
    "    recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "    rec.append(recall_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889aa1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:23:01.488483Z",
     "start_time": "2023-10-25T21:23:01.383222Z"
    }
   },
   "outputs": [],
   "source": [
    "x = range(1,101,5)\n",
    "plt.title('Accuracy vs n_estimators')\n",
    "plt.plot(x, acc, label = 'Accuracy')\n",
    "plt.plot(x, prec, label = 'Precision')\n",
    "plt.plot(x, rec, label = 'Recall')\n",
    "plt.scatter(x, acc, zorder = 5)\n",
    "plt.scatter(x, prec,  zorder = 5)\n",
    "plt.scatter(x, rec,  zorder = 5)\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31f4f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-08T14:10:05.554930Z",
     "start_time": "2023-10-08T14:10:05.550119Z"
    }
   },
   "source": [
    "`n_estimators` above 18 seem to be adequate. The default value of 100 does not lead to long compute time. Thus I will stick with the default value.\n",
    "\n",
    "Performance metrics for `RandomForestClassifier`:\n",
    "| Metric    | Value |\n",
    "|-----------|-------|\n",
    "| Accuracy  | 0.930 |\n",
    "| Precision | 0.930 |\n",
    "| Recall    | 0.930 |\n",
    "| F1 Score  | 0.930 |\n",
    "\n",
    "\n",
    "#### Confusion Matrix for Random forest:\n",
    "|            | BARBUNYA | BOMBAY | CALI | DERMASON | HOROZ | SEKER | SIRA | **Total (Actual)**   |\n",
    "|------------|----------|--------|------|----------|-------|-------|------|-------------|\n",
    "| BARBUNYA   | 246      | 0      | 16   | 0        | 0     | 1     | 7    | **270**     |\n",
    "| BOMBAY     | 0        | 103    | 0    | 0        | 0     | 0     | 0    | **103**     |\n",
    "| CALI       | 11       | 0      | 315  | 0        | 5     | 0     | 2    | **333**     |\n",
    "| DERMASON   | 0        | 0      | 0    | 663      | 1     | 4     | 37   | **705**     |\n",
    "| HOROZ      | 1        | 0      | 3    | 1        | 372   | 0     | 9    | **386**     |\n",
    "| SEKER      | 3        | 0      | 0    | 10       | 0     | 382   | 10   | **405**     |\n",
    "| SIRA       | 0        | 0      | 0    | 50       | 7     | 8     | 456  | **521**     |\n",
    "| **Total (Predicted)**  | **261**  | **103**|**334**|**724**  |**385**|**395**|**521**|**2723**    |\n",
    "\n",
    "#### Lastly, I will check if our model has overfitting issues:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e0e8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:24:13.392304Z",
     "start_time": "2023-10-25T21:24:13.045061Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_rf_train = rand_for.predict(X_train)\n",
    "\n",
    "display(Markdown(\"####Weighted performance metrics on training set: \"))\n",
    "# Calculate and print accuracy\n",
    "accuracy_rf_train = accuracy_score(y_train, y_pred_rf_train)\n",
    "print(\"Accuracy:\", round(accuracy_rf_train, 3))\n",
    "\n",
    "# Calculate and print precision\n",
    "precision_rf_train = precision_score(y_train, y_pred_rf_train, average='weighted', zero_division=1)\n",
    "print(\"Precision:\", round(precision_rf_train,3))\n",
    "\n",
    "# Calculate and print recall\n",
    "recall_rf_train = recall_score(y_train, y_pred_rf_train, average='weighted')\n",
    "print(\"Recall:\", round(recall_rf_train,3))\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1_rf_train = f1_score(y_train, y_pred_rf_train, average='weighted')\n",
    "print(\"F1 Score:\", round(f1_rf_train,3), '\\n')\n",
    "\n",
    "#Calculate the confusion matrix\n",
    "confus_rf_train = confusion_matrix(y_train, y_pred_rf_train, normalize=\"pred\")\n",
    "display(Markdown(\"#### Precision Confusion Matrix on training set: \"))\n",
    "disp = ConfusionMatrixDisplay(confus_rf_train, display_labels=kNN.classes_)\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6a250",
   "metadata": {},
   "source": [
    "We see here also that our model has overfitted badly. This means we need to adjust the different parameters.\n",
    "I will investigate `max_depth` again to see if I can make the model generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825f5f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:24:22.192792Z",
     "start_time": "2023-10-25T21:24:22.187879Z"
    }
   },
   "outputs": [],
   "source": [
    "depth = rand_for.max_depth\n",
    "max_depths = [tree.tree_.max_depth for tree in rand_for.estimators_]\n",
    "actual_max_depth = max(max_depths)\n",
    "print(\"Model depth for Random Forest mode is: \", actual_max_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ee709",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:25:26.319533Z",
     "start_time": "2023-10-25T21:24:22.981798Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_dt_depth = []\n",
    "accuracy_dt_depth_test = []\n",
    "depth = []\n",
    "for i in range(1,27):\n",
    "    depth.append(i)\n",
    "    rf_temp = RandomForestClassifier(criterion='entropy', max_depth=i)\n",
    "    rf_temp = rf_temp.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train_temp_rf = rf_temp.predict(X_train)\n",
    "    y_pred_test_temp_rf = rf_temp.predict(X_test)\n",
    "    \n",
    "    accuracy_dt_depth_val = accuracy_score(y_train, y_pred_train_temp_rf)\n",
    "    accuracy_dt_depth_test_val = accuracy_score(y_test, y_pred_test_temp_rf)\n",
    "    accuracy_dt_depth.append(accuracy_dt_depth_val)\n",
    "    accuracy_dt_depth_test.append(accuracy_dt_depth_test_val)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25268f94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:25:29.261985Z",
     "start_time": "2023-10-25T21:25:29.127793Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(depth, accuracy_dt_depth, label = 'Training set')\n",
    "plt.plot(depth, accuracy_dt_depth_test, label = 'Test set')\n",
    "plt.scatter(depth, accuracy_dt_depth, zorder = 5)\n",
    "plt.scatter(depth, accuracy_dt_depth_test, zorder = 5)\n",
    "plt.title(\"Train and Test set Accuracy vs RF Model depth\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('Model accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf497ab",
   "metadata": {},
   "source": [
    "We see that the `max_depth` of 6 seems to give the best result, as after that the accuracies begin to diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d34156f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:25:33.945981Z",
     "start_time": "2023-10-25T21:25:32.299643Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rand_for = RandomForestClassifier(criterion = 'gini', max_depth=6)\n",
    "rand_for = rand_for.fit(X_train, y_train)\n",
    "\n",
    "display(Markdown(\"#### Model performance metric after optimization: \"))\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rand_for.predict(X_test)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Accuracy:\", round(accuracy_rf, 3))\n",
    "\n",
    "# Calculate and print precision\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average='weighted', zero_division=1)\n",
    "print(\"Precision:\", round(precision_rf,3))\n",
    "\n",
    "# Calculate and print recall\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "print(\"Recall:\", round(recall_rf,3))\n",
    "\n",
    "# Calculate and print F1 score\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "print(\"F1 Score:\", round(f1_rf,3), '\\n')\n",
    "\n",
    "#Calculate the confusion matrix\n",
    "confus_rf = confusion_matrix(y_test, y_pred_rf, normalize=\"pred\")\n",
    "display(Markdown(\"#### Precision Confusion Matrix after optimization: \"))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confus_rf, display_labels=kNN.classes_)\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9495a087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-09T11:14:21.540566Z",
     "start_time": "2023-10-09T11:14:21.532866Z"
    }
   },
   "source": [
    "#### Model Metrics:\n",
    "\n",
    "| Metric    | Value |\n",
    "|-----------|-------|\n",
    "| Accuracy  | 0.907 |\n",
    "| Precision | 0.909 |\n",
    "| Recall    | 0.907 |\n",
    "| F1 Score  | 0.907 |\n",
    "\n",
    "#### Confusion Matrix:\n",
    "\n",
    "|            | BARBUNYA | BOMBAY | CALI | DERMASON | HOROZ | SEKER | SIRA | **Total (Actual)**   |\n",
    "|------------|----------|--------|------|----------|-------|-------|------|-------------|\n",
    "| BARBUNYA   | 211      | 0      | 45   | 0        | 0     | 1     | 13   | **270**     |\n",
    "| BOMBAY     | 0        | 103    | 0    | 0        | 0     | 0     | 0    | **103**     |\n",
    "| CALI       | 29       | 0      | 301  | 0        | 3     | 0     | 0    | **333**     |\n",
    "| DERMASON   | 0        | 0      | 0    | 660      | 0     | 4     | 41   | **705**     |\n",
    "| HOROZ      | 0        | 0      | 10   | 2        | 360   | 0     | 14   | **386**     |\n",
    "| SEKER      | 4        | 0      | 0    | 12       | 0     | 372   | 17   | **405**     |\n",
    "| SIRA       | 0        | 0      | 2    | 48       | 3     | 5     | 463  | **521**     |\n",
    "| **Total (Predicted)**  | **244**  | **103**|**358**|**722**  |**366**|**382**|**548**|**2723**    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d5990b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:25:40.021332Z",
     "start_time": "2023-10-25T21:25:39.838007Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = kNN.predict(X_test)\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\", zero_division=1)\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "model_accuracy = [accuracy, accuracy_dtree_gini, accuracy_logistic, accuracy_rf]\n",
    "model_precision = [precision, precision_dtree_gini, precision_logistic, precision_rf]\n",
    "model_recall = [recall, recall_dtree_gini, recall_logistic, recall_rf]\n",
    "model_f1 = [f1, f1_dtree_gini, f1_logistic, f1_rf]\n",
    "\n",
    "models = ['k-Nearest Neighbour', 'Decision Tree', 'Logistic Regression', 'Random Forest']\n",
    "\n",
    "plt.scatter(models, model_f1, zorder = 5, color = 'r')\n",
    "plt.scatter(models, model_accuracy, zorder = 5, color = 'b')\n",
    "plt.scatter(models, model_precision, zorder = 5, color = 'g')\n",
    "plt.scatter(models, model_recall, zorder = 5, color = 'orange')\n",
    "plt.title(\"Test Performance Metrics of Various Models\")\n",
    "plt.ylabel('Performance Rate')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a1cdf",
   "metadata": {},
   "source": [
    "\n",
    "| Metric    | `kNeightboursClassifier` | `DecisionTreeClassifier` | `LogisticRegression` | `RandomForestClassifier` |\n",
    "|-----------|-------|-------|-------|-------|\n",
    "| **Accuracy** | 0.930 |0.889 |0.927 | 0.907 |\n",
    "| **Precision**| 0.931 |0.894 |0.928 | 0.910 |\n",
    "| **Recall**   | 0.930 |0.889 |0.927 | 0.907 |\n",
    "| **F1 Score** | 0.930 |0.889 |0.927 | 0.907 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0153583d",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "For our given dataset of `Dry_Bean_Dataset.csv` we see that the `kNeighboursClassifier` had the best performance. It is also worth noting that `LogisticRegression` was not far behind. It is possible that different hold-out tests may change the results. Regardless, we see that `DecisionTreeClassifier` had the worst performance. \n",
    "\n",
    "In terms of different classes, we note that Bombay beans were consistently the most accurate as they were being classed correctly 100% of the time. This could be down to how distinct this class of beans are compared to other beans, making them easily distinguishable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0fbe65",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "Imagine that one of the bean types (Sira) is moderately poisonous. How should you nudge the performance of a classifier to address this? What evaluation metric is appropriate to capture this? Starting with the research resources linked below, identify a method to address this issue; test this method on the dataset. You dont need to get perfect accuracy on the Sira classification, the objective is to improve performance on the Sira class without too much impact on the other classes. Discuss your findings in markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa969b",
   "metadata": {},
   "source": [
    "#### Strategy:\n",
    "\n",
    "Since we are told that 'Sira' is slightly poisonous, we want to make sure that we classify as much of this class of beans correctly as possible. This means we want to minimize 'False Negative' (Type II error) results for 'Sira', where we misidentify them for something harmless. Also, it is more acceptable to classify other harmless beans as 'Sira', as the consequences are not as serious. But regardless, we want to make sure this scenario is kept low.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79b366",
   "metadata": {},
   "source": [
    "**Model performance for Sira class:**\n",
    "\n",
    "|**Metric**|**Value**|\n",
    "|----------|---------|\n",
    "|Accuracy  |0.930    |\n",
    "|Precision |0.864    |\n",
    "|Recall    |0.891    |\n",
    "|F1-score  |0.877    |\n",
    "\n",
    "Remember that the cost of misclassification is big for 'Sira'. So, we want to maximize Recall for this class and minimize the false negatives as mentioned earlier.\n",
    "\n",
    "For this problem, we will use `RandomForestClassifier` as it has a handy built in parameter called `class_weight` for imbalanced learning. Here, we can attribute more weight to the Sira class, making false-negatives less likely to occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb577bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:25:55.739511Z",
     "start_time": "2023-10-25T21:25:55.532610Z"
    }
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Let's remind ourselves what the metrics for Random Forest were for each classes.\"))\n",
    "\n",
    "model_metrics_rf = classification_report(y_test, y_pred_rf, target_names=kNN.classes_)\n",
    "print(model_metrics_rf)\n",
    "\n",
    "    \n",
    "confus_rf = confusion_matrix(y_test, y_pred_rf, normalize=\"pred\")\n",
    "display(Markdown(\"#### Precision Confusion Matrix Random Forest: \"))\n",
    "disp = ConfusionMatrixDisplay(confus_rf, display_labels=kNN.classes_)\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9365ef71",
   "metadata": {},
   "source": [
    "Let's add a weight to the `SIRA` class. By default, each class has weight of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55019c0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:26:01.150080Z",
     "start_time": "2023-10-25T21:25:59.685194Z"
    }
   },
   "outputs": [],
   "source": [
    "class_weights = {\n",
    "    'BARBUNYA': 1,\n",
    "    'BOMBAY': 1,\n",
    "    'CALI': 1,\n",
    "    'DERMASON': 1,\n",
    "    'HOROZ': 1,\n",
    "    'SEKER': 1,\n",
    "    'SIRA': 10  # giving higher weight to SIRA\n",
    "}\n",
    "\n",
    "rand_for_weighted = RandomForestClassifier(criterion = 'gini', max_depth=6, class_weight=class_weights)\n",
    "rand_for_weighted = rand_for_weighted.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66452b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:26:01.310016Z",
     "start_time": "2023-10-25T21:26:01.151209Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"### Let's see how the metrics look after we assign higher weight to SIRA class.\"))\n",
    "y_pred_rf_weighted = rand_for_weighted.predict(X_test)\n",
    "\n",
    "model_metrics_rf_weighted = classification_report(y_test, y_pred_rf_weighted, target_names=kNN.classes_)\n",
    "print(model_metrics_rf_weighted)\n",
    "    \n",
    "confus_rf_weighted = confusion_matrix(y_test, y_pred_rf_weighted, normalize=\"pred\")\n",
    "display(Markdown(\"#### Precision Confusion Matrix for Weighted Classes: \"))\n",
    "disp = ConfusionMatrixDisplay(confus_rf_weighted, display_labels=kNN.classes_)\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ffa878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:26:19.763581Z",
     "start_time": "2023-10-25T21:26:19.587379Z"
    }
   },
   "outputs": [],
   "source": [
    "confus_rf_weighted_recall = confusion_matrix(y_test, y_pred_rf_weighted, normalize=\"true\")\n",
    "display(Markdown(\"#### Recall Confusion Matrix for Weighted Classes: \"))\n",
    "disp = ConfusionMatrixDisplay(confus_rf_weighted_recall, display_labels=kNN.classes_)\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370517d",
   "metadata": {},
   "source": [
    "We see that the recall for the `SIRA` class has increased to 97%. This means we are correctly identifying 97% of the `SIRA` beans. However, this has come at a steep cost to other classes and overall model precision. The recall for `DERMASON` class has gone down to 75% from a high of 93%, while the `SIRA` precision is now a low 66% from 85%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71743bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:27:22.342769Z",
     "start_time": "2023-10-25T21:27:22.285479Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"### Let's compare the overall results for the two models\"))\n",
    "display(Markdown('<div class=\"alert alert-block alert-warning\"><b>Note: The accuracy score is averaged across all classes.</b></div>'))\n",
    "\n",
    "accuracy_rf_weighted = accuracy_score(y_pred_rf_weighted, y_test)\n",
    "precision_rf_weighted = precision_score(y_pred_rf_weighted, y_test, average=\"weighted\")\n",
    "recall_rf_weighted = recall_score(y_pred_rf_weighted, y_test, average=\"weighted\")\n",
    "f1_rf_weighted = f1_score(y_pred_rf_weighted, y_test, average=\"weighted\")\n",
    "\n",
    "\n",
    "table = f\"\"\"\n",
    "| Metric  | Not Weighted              | Weighted (weight = 10)                 |\n",
    "| ------- | -------                   | -------                                |\n",
    "|Accuracy |{round(accuracy_rf, 3)}    |   {round(accuracy_rf_weighted,3)}      |\n",
    "|Precision|{round(precision_rf,3)}|   {round(precision_rf_weighted,3)} |\n",
    "|Recall   |{round(recall_rf,3)}   |   {round(recall_rf_weighted,3)}    |\n",
    "|F1       |{round(f1_rf,3)}       |   {round(f1_rf_weighted,3)}        |\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52397ed2",
   "metadata": {},
   "source": [
    "### Remember that:\n",
    "\n",
    "$\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$\n",
    "\n",
    "$\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$\n",
    "\n",
    "$\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$\n",
    "\n",
    "and we wanted to minimize the FN or 'false-negative' rate for the SIRA class. This means, that for small values of FN, we get 'Recall' to be close to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bcc20a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:27:25.074212Z",
     "start_time": "2023-10-25T21:27:25.067603Z"
    }
   },
   "outputs": [],
   "source": [
    "text = f\"\"\"Indeed. Our values for recall has changed from {round(recall_rf, 3)} to {round(recall_rf_weighted, 3)}. Let's now see how the weight influences the recall.\"\"\"\n",
    "display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf453924",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:27:47.636770Z",
     "start_time": "2023-10-25T21:27:26.785047Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_weights_collector = []\n",
    "precision_weights_collector = []\n",
    "recall_weights_collector = []\n",
    "f1_weights_collector = []\n",
    "x = []\n",
    "\n",
    "for i in range(1, 15):\n",
    "    x.append(i)\n",
    "    class_weights = {\n",
    "        'BARBUNYA': 1,\n",
    "        'BOMBAY': 1,\n",
    "        'CALI': 1,\n",
    "        'DERMASON': 1,\n",
    "        'HOROZ': 1,\n",
    "        'SEKER': 1,\n",
    "        'SIRA': i \n",
    "    }\n",
    "\n",
    "    rand_for_weighted = RandomForestClassifier(criterion = 'gini', max_depth=6, class_weight=class_weights)\n",
    "    rand_for_weighted = rand_for_weighted.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_pred_rf_weighted = rand_for_weighted.predict(X_test)\n",
    "\n",
    "\n",
    "    accuracy_rf_weighted = round(accuracy_score(y_test, y_pred_rf_weighted), 3)\n",
    "    accuracy_weights_collector.append(accuracy_rf_weighted)\n",
    "\n",
    "    precision_rf_weighted = round(precision_score(y_test, y_pred_rf_weighted, average=None, zero_division=1)[-1], 3)\n",
    "    precision_weights_collector.append(precision_rf_weighted)\n",
    "\n",
    "    recall_rf_weighted = round(recall_score(y_test, y_pred_rf_weighted, average=None, zero_division=1)[-1], 3)\n",
    "    recall_weights_collector.append(recall_rf_weighted)\n",
    "    \n",
    "    f1_rf_weighted = round(f1_score(y_test, y_pred_rf_weighted, average=None, zero_division=1)[-1], 3)\n",
    "    f1_weights_collector.append(f1_rf_weighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b001ebe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T21:28:16.265789Z",
     "start_time": "2023-10-25T21:28:16.103298Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, accuracy_weights_collector, label = \"Accuracy\")\n",
    "plt.plot(x, precision_weights_collector, label = \"Precision\")\n",
    "plt.plot(x, recall_weights_collector, label = \"Recall\")\n",
    "plt.plot(x, f1_weights_collector, label = \"F-1 score\")\n",
    "\n",
    "plt.scatter(x, accuracy_weights_collector)\n",
    "plt.scatter(x, precision_weights_collector)\n",
    "plt.scatter(x, recall_weights_collector)\n",
    "plt.scatter(x, f1_weights_collector)\n",
    "\n",
    "plt.title(\"Random Forest model metrics vs SIRA class weight\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('SIRA class_weight')\n",
    "plt.ylabel(\"Rate\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab5679e",
   "metadata": {},
   "source": [
    "After plotting the class_weight and the corresponding model performances, we can see the steep drop off for many of the metrics. It seems a class_weight of 4 is the most ideal, as the rate of change of recall slows down after that point on while precision is still above 70%.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "There is a clear trade-off between precision and recall when we increase the class weight for the SIRA class. This harmonic-mean is captured by the F-1 score, which is also decreasing with increased weight associated with the SIRA class. We are told that the SIRA class of beans are \"moderately\" poisonous. This makes it harder to know exactly where the acceptable false negative rate might lie and what false-positive rates for the other classes are also acceptable.\n",
    "In theory, we could keep increasing the class weight until we reach 100% recall rate for the poisonous class. However, we will end up marking innocent beans are being poisonous, thus drastically lowering the model precision. It would be nice to keep precision relatively high, but I guess we can't eat our cake and have it too... \n",
    "\n",
    "Throughout this notebook, I have shown multiple confusion matrices. The ones labeled as 'precision' matrices are normalized against prediction total, while the 'recall' matrices are normalized using the true total."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
